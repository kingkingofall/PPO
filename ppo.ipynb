{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance Sampling\n",
    "\n",
    "$E_{x\\sim p}[f(x)]\\approx \\frac{1}{N}\\sum^N_{i=1}f(x^i),\\color{red}{大数定律},当N趋向于无穷大时，等式成立$\n",
    "\n",
    "分布p的期望可以约等于从分布p进行大量采样后的数据的均值，但是目前我们无法直接从分布p进行采样，所以需要变形\n",
    "\n",
    "原式=$\\int f(x)p(x)dx=\\int f(x)\\frac{p(x)}{q(x)}q(x)dx=E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]\\approx \\frac{1}{N}\\sum^N_{i=1}f(x^i)\\frac{p(x^i)}{q(x^i)}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重要性采样可以保证采样数据的期望大致上符合分布p,但是还有方差的偏差问题,方差公式:\n",
    "\n",
    "$Var_{x\\sim p}[x]=E_{x\\sim p}[x^2]-(E_{x\\sim p}[x])^2$\n",
    "\n",
    "$Var_{x\\sim p}[f(x)]=E_{x\\sim p}[f(x)^2]-(E_{x\\sim p}[f(x)])^2$\n",
    "\n",
    "$Var_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]=E_{x\\sim q}[(f(x)\\frac{p(x)}{q(x)})^2]-(E_{x\\sim p}[f(x)\\frac{p(x)}{q(x)}])^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
